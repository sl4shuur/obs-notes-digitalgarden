---
{"dg-publish":true,"dg-path":"KSE/Optimization Techniques/13. PROJECTED GRADIENT DESCENT.md","permalink":"/kse/optimization-techniques/13-projected-gradient-descent/","tags":["kse","math/calculus"],"created":"2025-03-10T09:18:09.689+02:00","updated":"2025-03-11T21:42:11.303+02:00"}
---


# PROJECTED GRADIENT DESCENT

---

- **Gradient-based optimization** is widely used in various fields.
- Many problems include **constraints** that must be satisfied.
- **Projected Gradient Descent (PGD)** extends standard gradient descent by ensuring iterates remain feasible within a given constraint set.

> [!tip] Recall:
> Function $f(x)$ is **[[02 - KSE/2_5-term/Methods of Optimization/06. Smooth Functions#Strongly Convex Functions (Another Approach)\|strongly convex]]** with parameter $\textcolor{var(--color-yellow)}{\mu}$:
>
> $$
> f(y) \textcolor{var(--color-aqua)}{\geq} f(x) + (y - x)^T \nabla f(x) + \frac{\textcolor{var(--color-yellow)}{\mu}}{2} \|y - x\|^2 \quad \forall x, y \in \mathbb{R}^n
> $$
>
> Function $f(x)$ is called **[[02 - KSE/2_5-term/Methods of Optimization/06. Smooth Functions\|smooth]]** with parameter $\textcolor{var(--color-cyan)}{L}$
>
> $$
> f(y) \textcolor{var(--color-red)}{\leq} f(x) + \nabla f(x)^T (y - x) + \frac{\textcolor{var(--color-cyan)}{L}}{2} \|x - y\|^2 \quad \forall x, y \in \mathbb{R}^n
> $$

---

## Constrained Minimization

Let $f:D(f) \to \mathbb{R}$ be convex and let $X \subseteq D(f)$ be a convex set. A point $x \in X$ is a minimizer of $f$ over $X$ if:

$$
f(x) \leq f(y), \quad \forall y \in X.
$$

### Lemma 1

If $f:D(f) \to \mathbb{R}$ is convex and differentiable over an open domain $D(f) \subseteq \mathbb{R}^n$ and $X \subseteq D(f)$ is a convex set, then a point $x^* \in X$ is a minimizer of $f$ over $X$ if and only if:

$$
\nabla f(x^*)^T(x - x^*) \geq 0, \quad \forall x \in X.
$$

![Projected Gradient Descent.png|500](/img/user/assets/img/Projected%20Gradient%20Descent.png)

> [!tip] Recall:
> [[02 - KSE/2_5-term/Methods of Optimization/10. Existence of a Minimum\|Existence of a Minimum]] for constrained optimization.

---

## Definition of Projected Gradient Descent

Consider the constrained optimization problem:

$$
\min_{x \in X} f(x),
$$

where $X$ is a feasible convex set. The **Projected Gradient Descent (PGD)** iterates are given by:

$$
x_{k+1} = P_X(x_k - \gamma_k \nabla f(x_k)),
$$

where $P_X(\cdot)$ is the **[[02 - KSE/2_5-term/Methods of Optimization/09. Projections\|projection operator]]**.

---

## Projected Gradient Descent Algorithm

### Initialization

Choose an initial point $x_0 \in X$.

### Iterative Steps

For $k = 0, 1, 2, \dots$:

1. Compute the gradient:
   $$
   g_k = \nabla f(x_k)
   $$
2. Take a gradient step:
   $$
   y_k = x_k - \gamma_k g_k
   $$
3. Project onto the feasible set $X$:
   $$
   x_{k+1} = P_X(y_k)
   $$

**Repeat until convergence.**

---

## Theorem 1: Step Size Selection

If $f(x)$ is convex and $L$-[[02 - KSE/2_5-term/Methods of Optimization/06. Smooth Functions\|smooth]]

$$
\|\nabla f(x) - \nabla f(y)\| \leq L\|x - y\|,
$$

then the **Projected Gradient Method** converges for step sizes satisfying:

$$
0 < \gamma_k \leq \frac{1}{L}.
$$

A common choice is:

$$
\gamma_k = \frac{1}{L}.
$$

---

## Theorem 2: Convergence Rate

Let $f:\mathbb{R}^n \to \mathbb{R}$ be **[[02 - KSE/2_5-term/Methods of Optimization/03. Convex Functions\|convex]]** and differentiable, $X \subseteq \mathbb{R}^n$ closed and convex, and let $x^*$ be a minimizer of $f$ over $X$. Suppose that:

- $\|x_0 - x^*\| \leq R$ with $x_0 \in X$.
- $\|\nabla f(x)\| \leq B$ for all $x \in X$.

Where $B$ is the **[[02 - KSE/2_5-term/Methods of Optimization/07. Lipschitz Functions#Definition\|Lipschitz constant of the gradient]]**.

Choosing the constant step size:

$$
\gamma := \frac{R}{B\sqrt{K}},
$$

Projected Gradient Descent yields:

$$
\frac{1}{K} \sum_{t=0}^{K-1} (f(x_k) - f(x^*)) \leq \frac{RB}{\sqrt{K}}.
$$

Looks the same as the **[[02 - KSE/2_5-term/Methods of Optimization/11. GRADIENT DESCENT#Theorem 1 Bounded Gradient\|Gradient Descent convergence rate]]**!

---

## Theorem 3: Convergence Rate (Smooth Case)

Let $f:\mathbb{R}^n \to \mathbb{R}$ be convex and differentiable, and let $X \subseteq \mathbb{R}^n$ be a **closed convex set**. Suppose there is a minimizer $x^*$ of $f$ over $X$ and that $f$ is **smooth over $X$** with parameter $L$. Choosing the step size:

$$
\gamma := \frac{1}{L},
$$

Projected Gradient Descent yields:

$$
f(x_K) - f(x^*) \leq \frac{L}{2K} \|x_0 - x^*\|^2, \quad K > 0.
$$

Looks similar to the **[[02 - KSE/2_5-term/Methods of Optimization/11. GRADIENT DESCENT#Theorem 3 Smooth and Convex Functions\|Gradient Descent smooth case]]**!

---

## Theorem 4: Error Bound

If $f:\mathbb{R}^n \to \mathbb{R}$ is convex and differentiable, and $f$ is **$L$-smooth** and **strongly convex** with parameter $\mu > 0$, then choosing:

$$
\gamma := \frac{1}{L},
$$

Projected Gradient Descent satisfies:

1. **Geometric Decrease in Distance to $x^*$:**

   $$
   \|x_{k+1} - x^*\|^2 \leq \left(1 - \frac{\mu}{L} \right) \|x_k - x^*\|^2, \quad k \geq 0.
   $$

2. **Exponential Decrease in Function Value:**

   $$
   f(x_K) - f(x^*) \leq \frac{L}{2} \left(1 - \frac{\mu}{L} \right)^K \|x_0 - x^*\|^2, \quad K > 0.
   $$

Thus, **error decreases exponentially** as iterations proceed.

---

## Summary:

- **Projected Gradient Descent (PGD)** is an extension of gradient descent for constrained problems.
- It ensures that iterates **remain feasible** within the constraint set $X$.
- The **projection step** ensures valid updates.
- Step sizes are chosen to **ensure convergence** based on properties of the function (e.g., smoothness).
- **Convergence rate depends on problem smoothness and strong convexity**.

PGD is widely used in **optimization, machine learning, and constrained convex problems**. ðŸš€
