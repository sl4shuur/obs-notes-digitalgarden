---
{"dg-publish":true,"dg-path":"KSE/Optimization Techniques/06. Smooth Functions.md","permalink":"/kse/optimization-techniques/06-smooth-functions/","tags":["kse","math/calculus"],"created":"2025-03-09T14:48:22.157+02:00","updated":"2025-03-09T15:48:41.734+02:00"}
---

# Smooth Functions

---  

A **smooth function** is a differentiable function that does not change too abruptly. It is controlled by a **smoothness parameter** $L$, which bounds how fast the gradient can change. Unlike convexity, **smoothness does not require the function to be convex**, but it provides useful guarantees about how the function behaves.  

## Definition  

A function $f: D(f) \to \mathbb{R}$ is **smooth** with parameter $L$ over $\Omega \subseteq D(f)$ if:  

$$
f(y) \leq f(x) + \nabla f(x)^T (y - x) + \frac{L}{2} \|x - y\|^2 \quad \forall x, y \in D(f).
$$  

This inequality ensures that the function is **not too steep**, meaning that the graph of $f$ is bounded above by a **quadratic approximation** at every point.  

> [!summary] Note
> This formula is derived from the **second-order Taylor expansion** of $f$ around $x$, with an additional quadratic term $\frac{L}{2} \|x - y\|^2$ that bounds the function's growth.
>
> So, basically, instead of $\frac{1}{2} (y - x)^T H_f(x) (y - x)$, where $H_f(x)$ is the Hessian matrix (second derivative for **Taylor expansion**), we use $\frac{L}{2} \|x - y\|^2$ â€” the **smoothness parameter**.
>
> The smoothness parameter $L$ controls how rapidly the function's gradient can change, ensuring it does not grow too steeply.
>
> This **quadratic term** $\frac{L}{2} \|x - y\|^2$ acts as a **global upper bound** on how much $f(y)$ can exceed the first-order approximation $f(x) + \nabla f(x)^T (y - x)$.

> [!REMARK] Remark
> This definition **does not require convexity**, meaning a function can be smooth even if it is not convex.

## Geometric Intuition  

At any point $x$, the graph of $f$ lies **below** a **quadratic upper bound** (a paraboloid). Thus, the function does not grow too rapidly, as illustrated by the tangent paraboloid in the diagram.

![Smooth Function.png|600](/img/user/assets/Smooth%20Function.png)

---

## Operations That Preserve Smoothness  

Just like convexity, smoothness is **preserved under specific operations**:  

### 1. Linear Combination of Smooth Functions  

If $f_1, \dots, f_m$ are smooth functions with parameters $L_1, L_2, \dots, L_m$ and scalars $\lambda_1, \dots, \lambda_m \in \mathbb{R}_+$, then:  

$$
f := \sum_{i=1}^{k} \lambda_i f(x_i)
$$  

is **smooth** with parameter:  

$$
L = \sum_{i=1}^{m} \lambda_i L_i.
$$  

This means that taking a weighted sum of smooth functions **preserves smoothness**, and the new smoothness parameter is just the sum of the individual parameters.  

### 2. Composition with an Affine Transformation  

If $f$ is smooth with parameter $L$, and:  

$$
g(x) = Ax + b
$$  

where $A$ is an $n \times n$ matrix and $b$ is a vector, then the **composite function** $f \circ g$ is **smooth** with parameter:  

$$
L \|A\|^2
$$  

where $\|A\|$ is the **spectral norm** of $A$.  

This means that applying a <strong><span style="color: var(--color-purple);">linear transformation</span></strong> to a smooth function still results in a smooth function, but the smoothness parameter gets scaled by the <strong><span style="color: var(--color-aqua);">square of the spectral norm</span></strong>.  

---

## Characterization of Smooth Functions (Lemma)

If $f(x): \mathbb{R}^n \to \mathbb{R}$ is **convex and differentiable**, then the following two statements are **equivalent**:  

1. $f$ is smooth with parameter $L$.  
2. The gradient of $f$ is **Lipschitz continuous**:  

   $$  
   \|\nabla f(x) - \nabla f(y)\| \leq L \|x - y\| \quad \forall x, y \in \mathbb{R}^n.
   $$  

This means that the gradient of a smooth function <strong><span style="color: var(--color-aqua);">does not change too abruptly</span></strong>, making optimization methods like gradient descent more stable.  

---

## Strongly Convex Functions (Another Approach)

In addition to being **smooth**, a function can also be [[02 - KSE/2_5-term/Methods of Optimization/03. Convex Functions#Strictly Convex Functions\|strongly convex]]. Strong convexity strengthens the usual convexity condition by ensuring that the function has a **uniform curvature**, which guarantees **faster convergence** in optimization algorithms.  

A function that is both **strongly convex and smooth** enjoys <strong><span style="color: var(--color-aqua);">better optimization properties</span></strong> compared to standard convex functions.  

> [!tip]+ Recall from the image:
> ![Strict convex.png](/img/user/assets/Strict%20convex.png)

---

## Definition (close to L-smooth)

A function $f: \mathbb{R}^n \to \mathbb{R}$ is called **$l$-strongly convex** if, for all $x, y$:  

$$
f(y) \geq f(x) + (y - x)^T \nabla f(x) + \frac{l}{2} \|y - x\|^2.
$$  

This condition ensures that $f$ **curves upward at a minimum rate of $l$**, preventing <strong><span style="color: var(--color-cyan);">flat regions</span></strong> that could slow down optimization.  

> [!summary] Comparing this definition with the smoothness condition:
>
> $$
> f(y) \textcolor{var(--color-aqua)}{\geq} f(x) + (y - x)^T \nabla f(x) + \frac{\textcolor{var(--color-yellow)}{l}}{2} \|y - x\|^2.
> $$
> $$
> f(y) \textcolor{var(--color-red)}{\leq} f(x) + \nabla f(x)^T (y - x) + \frac{\textcolor{var(--color-cyan)}{L}}{2} \|x - y\|^2,
> $$

So, simply put, **strong convexity** is a <strong><span style="color: var(--color-aqua);">lower bound</span></strong>, while <strong><span style="color: var(--color-cyan);">smoothness</span></strong> is an <strong><span style="color: var(--color-red);">upper bound</span></strong> on how much the function can grow.

---

## Strong Convexity and Smoothness Together  

If a function $f$ is both **$l$-strongly convex** and **$L$-smooth** (as defined earlier), then it satisfies the following **sandwich inequality**:  

$$
\textcolor{var(--color-yellow)}{f(x) + (y - x)^T \nabla f(x) + \frac{l}{2} \|x - y\|^2}

\leq
f(y)
\leq

\textcolor{var(--color-cyan)}{f(x) + (y - x)^T \nabla f(x) + \frac{L}{2} \|x - y\|^2}
$$  

This means that the function is *trapped between two quadratic approximations*â€”one defined by **$l$ (strong convexity)** and one by $L$ (<strong><span style="color: var(--color-cyan);">smoothness</span></strong>).  

Once again:

- The <strong><span style="color: var(--color-aqua);">lower bound</span></strong> (left inequality) guarantees that the function is **at least as curved as a quadratic function with curvature $l$**.  
- The <strong><span style="color: var(--color-red);">upper bound</span></strong> (right inequality) guarantees that the function **does not grow faster than a quadratic function with curvature $L$**. 

---

## Summary  

| Concept                                   | Meaning                                                                     |
| ----------------------------------------- | --------------------------------------------------------------------------- |
| **Smooth function**                       | A differentiable function whose gradient does not change too fast.          |
| **Smoothness condition**                  | $f(y) \leq f(x) + \nabla f(x)^T (y - x) + \frac{L}{2} \|x - y\|^2$.         |
| **Smoothness does not require convexity** | A function can be smooth without being convex.                              |
| **Lipschitz gradient**                    | The gradient must satisfy $\|\nabla f(x) - \nabla f(y)\| \leq L \|x - y\|$. |
| **Preservation**                          | Smoothness is preserved under weighted sums and affine transformations.     |
| **$l$-strongly convex**                   | The function **curves upwards uniformly**, ensuring no flat regions.        |
| **$l$-strong convexity + $L$-smoothness** | The function is **bounded between two quadratic approximations**.           |

Smooth functions are important in optimization because they **ensure that gradients behave predictably**, making them useful in gradient-based methods. ðŸš€

In short, a function that is both **strongly convex and smooth** is **easier to optimize**, enjoys **better theoretical guarantees**, and ensures **efficient gradient-based optimization**. ðŸš€
