---
{"dg-publish":true,"dg-path":"KSE/Optimization Techniques/12. STEEPEST GRADIENT DESCENT.md","permalink":"/kse/optimization-techniques/12-steepest-gradient-descent/","tags":["kse","math/calculus"],"created":"2025-03-10T08:30:00.657+02:00","updated":"2025-03-11T21:42:11.263+02:00"}
---


# STEEPEST GRADIENT DESCENT

---

**Steepest gradient descent** is a variation of the **[[02 - KSE/2_5-term/Methods of Optimization/11. GRADIENT DESCENT\|gradient descent]]** method where the step size is **not fixed** but instead chosen <strong><span style="color: var(--color-cyan);">dynamically</span></strong> at each iteration. This approach ensures that the algorithm takes the <strong><span style="color: var(--color-green);">most efficient step</span></strong> towards the minimum in each iteration.

## Update Rule

The update rule for steepest gradient descent is given by:

$$
x^{k+1} = x^k - \textcolor{var(--color-green)}{\gamma_k} \nabla f(x^k),
$$

where the step size $\textcolor{var(--color-green)}{\gamma_k}$ is chosen to minimize the function along the descent direction:

$$
\textcolor{var(--color-green)}{\gamma_k} = \arg\min_{\gamma \, > \, 0} f(x^k - \gamma \nabla f(x^k))

\quad

\textcolor{var(--color-cyan)}{(\ast)}
$$

This means that **at each iteration**, we find the optimal step size $\textcolor{var(--color-green)}{\gamma_k}$ by minimizing the function along the direction of the negative gradient.

---

## Geometric Interpretation

![Steepest gradient descent.png|700](/img/user/assets/img/Steepest%20gradient%20descent.png)

The two images illustrate the difference between **[[02 - KSE/2_5-term/Methods of Optimization/11. GRADIENT DESCENT\|fixed step size]]** gradient descent and <strong><span style="color: var(--color-green);">steepest gradient descent</span></strong>:

1. **Fixed Step Size in Gradient Descent**:

   - The step size is constant across all iterations, leading to **potential inefficiencies** in convergence.

2. **Adaptive Step Size in Steepest Gradient Descent**:
   - The step size is chosen optimally at each iteration, ensuring **more efficient movement** towards the minimum.

---

## Convergence Properties

<strong><span style="color: var(--color-purple);">Theorem:</span></strong>
Let $\{x^k\}$ be a convergent sequence generated by the steepest descent algorithm applied to a function $f$. Then, in the **worst case**, the order of convergence of $\{x^k\}$ is **1**.

This means that in the worst case, steepest gradient descent has a **linear rate of convergence**.

<strong><span style="color: var(--color-aqua);">What Does This Mean?</span></strong>

- The **order of convergence** describes how fast the error decreases as the algorithm progresses.
- If the order of convergence is **1**, it means the error **decreases linearly** in the worst case.
- This implies that <strong><span style="color: var(--color-green);">steepest gradient descent</span></strong> is <strong><span style="color: var(--color-red);">not always very fast</span></strong>—it may take many iterations to reach an accurate solution, depending on the function.

---

## Modification: Step Size Reduction (Step Shredding)

A practical modification of steepest gradient descent involves **step size reduction**, also called **step shredding**. Here, the step size is adjusted dynamically based on a condition:

$$
f(x^{k+1}) \leq f(x^k) - \varepsilon \gamma \|\nabla f(x^k)\|^2 \quad \textcolor{var(--color-cyan)}{(\ast\ast)}
$$

where $\varepsilon \in (0,1)$ is a pre-selected _method parameter_, that controls the **minimum required decrease** in function value.

> [!Remark] Remark
> Very often, $\varepsilon$ is set to a small value, such as $0.1$

This condition ensures that **each step** results in sufficient function <strong><span style="color: var(--color-red);">decrease</span></strong>.

If the condition is <strong><span style="color: var(--color-orange);">not satisfied</span></strong>, we reduce the step size $\textcolor{var(--color-green)}{\gamma_k}$ using a **reduction factor** $\delta \in (0,1)$:

$$
\textcolor{var(--color-aqua)}{\gamma_k} = \delta \textcolor{var(--color-green)}{\gamma_k}.
$$

The parameter **$\delta$** determines how **aggressively** the step size is reduced when the descent condition is not met.

> [!Remark] Remark
> Very often, the reduction factor $\delta$ is set to a small value, such as $0.5$.

<strong><span style="color: var(--color-aqua);">Why is this modification useful?</span></strong>

- If $\gamma_k$ is too **large**, we may **overshoot** the minimum, leading to oscillations or divergence.
- If $\gamma_k$ is too **small**, convergence becomes **slow** and inefficient.
- The **step reduction rule** ensures we **adapt** to the function landscape, preventing large, ineffective updates while still making steady progress.

---

## Algorithm for Modified Steepest Descent

1. **Initialize** $x^0 \in \mathbb{R}^n$, an arbitrary step size $\gamma$ (*the same in all iterations*), and parameters $\varepsilon, \delta \in (0,1)$.
2. **For each iteration** $k+1$:

   Set initial $\textcolor{var(--color-green)}{\gamma_k} = \gamma$.
   Compute the update:

   $$
   x^{k+1} = x^k - \textcolor{var(--color-green)}{\gamma_k} \nabla f(x^k).
   $$

   Check if the inequality:

   $$
   f(x^{k+1}) \leq f(x^k) - \varepsilon \textcolor{var(--color-green)}{\gamma_k} \|\nabla f(x^k)\|^2
   $$

   is satisfied.
   **If satisfied:** accept $\textcolor{var(--color-green)}{\gamma_k}$ and move to the next iteration.
   **If not satisfied:** reduce step size $\textcolor{var(--color-aqua)}{\gamma_k} = \delta \textcolor{var(--color-green)}{\gamma_k}$ and repeat.

This method ensures that the step size is **sufficiently large for fast convergence** while also **ensuring descent in function value**.

---

## Practical Insights

- The modification using **step shredding** makes steepest gradient descent more robust.
- **Requirement** $\textcolor{var(--color-cyan)}{(\ast\ast)}$ is stricter than $\textcolor{var(--color-cyan)}{(\ast)}$, but both ensure function decrease.
- This version of **gradient descent is widely used in practice** due to its adaptive nature.

Steepest gradient descent dynamically **adapts the step size**, ensuring that the **optimal step length is chosen at each iteration**, leading to **faster and more efficient convergence**.

---

## Simple Example of the Algorithm

### Finding the Minimum of $f(x) = x^2$

Let’s minimize:

$$
f(x) = x^2
$$

using **Modified Steepest Descent**.

### Step 1: Initialize Parameters

- Choose an initial point: $x^0 = 5$
- Initial step size: $\gamma = 1$
- Set $\varepsilon = 0.1$, $\delta = 0.5$.

### Step 2: Iterate

1. **Compute Gradient**:  
   $$\nabla f(x) = 2x$$  
   At $x^0 = 5$:  
   $$\nabla f(5) = 10$$

2. **Compute Update**:  
   $$ x^1 = x^0 - \gamma \nabla f(x^0) = 5 - 1(10) = -5 $$

3. **Check Descent Condition**:  
   $$ f(x^1) \leq f(x^0) - \varepsilon \gamma \|\nabla f(x^0)\|^2 $$  
   $$ (-5)^2 \leq 5^2 - 0.1(1)(10^2) $$  
   $$ 25 \leq 25 - 10 $$  
   $$ 25 \leq 15 $$
   $$ \textcolor{var(--color-red)}{25 \nleq 15 }$$

Since the condition <strong><span style="color: var(--color-red);">fails</span></strong>, we **reduce** $\gamma$:  
$$\textcolor{var(--color-aqua)}{\gamma_1} = \delta \gamma = 0.5(1) = \textcolor{var(--color-aqua)}{0.5}$$  
Then **repeat the update with the smaller step size**.

---

### Step 3: New Update with Reduced $\gamma$

1. **Compute New Update**:  
   $$ x^1 = 5 - \textcolor{var(--color-aqua)}{0.5}(10) = 0 $$

2. **Check Descent Condition Again**:  
   $$ f(0) \leq 5^2 - 0.1(0.5)(10^2) $$  
   $$ 0 \leq 25 - 5 $$  
   $$ 0 \leq 20 $$
   $$ \textcolor{var(--color-green)}{0 \leq 20 }$$

Since the condition <strong><span style="color: var(--color-green);">holds</span></strong>, we **accept** $x^1 = 0$ and move to the next iteration.
