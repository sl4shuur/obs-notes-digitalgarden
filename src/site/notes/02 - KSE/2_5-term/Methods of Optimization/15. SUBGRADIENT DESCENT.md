---
{"dg-publish":true,"dg-path":"KSE/Optimization Techniques/15. SUBGRADIENT DESCENT.md","permalink":"/kse/optimization-techniques/15-subgradient-descent/","tags":["kse","math/calculus"],"created":"2025-03-10T09:56:49.551+02:00","updated":"2025-03-10T11:52:02.964+02:00"}
---


# SUBGRADIENT DESCENT

---

Subgradient descent is an extension of gradient descent for **non-smooth convex functions**. Unlike classical gradient descent, which requires differentiability, subgradient descent allows optimization over functions that have **kinks or discontinuities** in their gradients.

---

## The Subgradient Descent Algorithm

Given an initial point:

$$ x_0 \in \mathbb{R}^n $$

and a **[[02 - KSE/2_5-term/Methods of Optimization/08. Subgradient and subdifferential\|subgradient]]**:

$$ g \in \partial f(x), $$

the update rule follows:

$$ x_{k+1} := x_k - \gamma g_k, $$

where $g_k$ is **any** subgradient of $f(x)$ at $x_k$.

<strong><span style="color: var(--color-aqua);">Important Note:</span></strong>

Negative subgradients are <strong><span style="color: var(--color-red);">not necessarily descent directions</span></strong>.

![Subgradient Descent Does Not Descend.png](/img/user/assets/Subgradient%20Descent%20Does%20Not%20Descend.png)

![Negative subgradients are not necessarily descent directions.png](/img/user/assets/Negative%20subgradients%20are%20not%20necessarily%20descent%20directions.png)

This differs from classical gradient descent, where the negative gradient always points towards decreasing function values.

---

## Example of Subgradient Descent

Consider the function:

$$ f(x) = |x_1 - x_2| + 0.2 |x_1 + x_2| $$

with a subgradient at $(1,1)$:

$$ g(1,1) = (1.2, -0.8) $$

Since $f(x)$ is not smooth, **any** subgradient could be used at each step, leading to different update directions.

<strong><span style="color: var(--color-purple);">An important property:</span></strong>

$$ f(x_{k+1}) \geq f(x_k) $$

shows that subgradient descent <strong><span style="color: var(--color-red);">does not necessarily decrease function values</span></strong> at each step.

<strong><span style="color: var(--color-pink);">Additionally:</span></strong>

$$ \|x_{k+1} - x^*\| \leq \|x_k - x^*\| $$

suggests that the iterates remain <strong><span style="color: var(--color-green);">bounded</span></strong>, even if the function value does not strictly decrease.

---

## How to Choose a Step Size?

In subgradient descent, the step size $\gamma_k$ plays a crucial role. There are several common choices:

### 1. Diminishing Step Size:

A common approach is to use a sequence $\gamma_k$ that tends to zero:

$$ x_{k+1} = x_k - \gamma_k \frac{\partial^+ f(x_k)}{\|\partial^+ f(x_k)\|}, $$

> [!remark]+ Remark:
> The **plus sign ($+$)** in $\partial^+ f(x_k)$ indicates that we are selecting a **particular subgradient** from the subdifferential set $\partial f(x)$. There are several possible interpretations:
>
> 1. **Selecting the steepest subgradient**
>
>    - If multiple subgradients exist (e.g., at a nondifferentiable point), we might choose the one with the **largest norm** or the **steepest > descent direction**.
>    - This is useful in algorithms like **normalized subgradient descent** to ensure consistent step sizes.
>
> 2. **Selecting a specific element in a structured way**
>
>    - Some optimization methods require a deterministic or structured choice of subgradient.
>    - The plus sign may indicate a particular rule for selecting a subgradient, e.g., the **right-hand derivative** in one-dimensional cases.
>
> 3. **Right subgradient in nonsmooth analysis**
>    - In some contexts (like directional derivatives), $\partial^+ f(x)$ may refer to the **right subdifferential**, meaning we consider > subgradients corresponding to increasing values of $x$.

where $\gamma_k$ satisfies:

$$ \gamma_k \to 0, \quad \sum_{k=0}^{\infty} \gamma_k = \infty. $$

<strong><span style="color: var(--color-aqua);">Three typical choices:</span></strong>

- $\gamma_k = \frac{c}{k}$, where $c$ is a <strong><span style="color: var(--color-green);">constant</span></strong>.
- $\gamma_k = \frac{x_0}{k^\rho}, \quad 0 < \rho \leq 1$.
- $\gamma_k = \frac{\gamma_0}{k \|g_k\|}$.

### 2. If the optimal function value $f(x^*)$ is known:

The update rule can be modified as:

$$ x_{k+1} = x^k - \frac{\partial^+ f(x^k)}{\|\partial^+ f(x^k)\|^2} f(x^k) - f(x^*)$$

### 3. Using the best observed function value:

Define:

$$ f_k^{\text{best}} = \min_{1 \leq i \leq k} f(x_i), $$

and use:

$$ f^* = \min_k f(x_k). $$

This ensures convergence by tracking the best function value observed so far.

---

## Convergence of Subgradient Descent

### Lipschitz Convex Functions

If $f: \mathbb{R}^n \to \mathbb{R}$ is **convex and [[02 - KSE/2_5-term/Methods of Optimization/07. Lipschitz Functions#Definition\|B-Lipschitz continuous]]** with a global minimum $x^*$, and:

$$ \|x_0 - x^*\| \leq R, $$

then choosing the constant step size:

$$ \gamma := \frac{R}{B \sqrt{T}} $$

yields:

$$ \frac{1}{T} \sum_{t=0}^{T-1} f(x_t) - f(x^*) \leq \frac{RB}{\sqrt{T}}. $$

### Strongly Convex Functions

If $f: \mathbb{R}^n \to \mathbb{R}$ is **[[02 - KSE/2_5-term/Methods of Optimization/06. Smooth Functions#Strongly Convex Functions (Another Approach)\|strongly convex]]** with parameter $\mu > 0$, then using the decreasing step size:

$$ \gamma_k = \frac{2}{\mu (k+1)}, \quad t > 0, $$

yields:

$$ f\left(\frac{2}{K(K+1)} \sum_{i=1}^{K} i x_i\right) - f(x^*) \leq \frac{2B^2}{\mu(K+1)}. $$

where:

$$ B = \max_k \|g_k\|. $$

This shows that subgradient descent **converges slower than standard gradient descent**, but still guarantees progress over time.

---

## Subgradient Descent for Constrained Optimization

The **[[02 - KSE/2_5-term/Methods of Optimization/13. PROJECTED GRADIENT DESCENT\|projected subgradient method]]** is used for constrained optimization, where $x$ must remain within a feasible set $X$:

$$ \min f(x), \quad X \subseteq D(f). $$

The update step includes a **[[02 - KSE/2_5-term/Methods of Optimization/09. Projections\|projection]]** onto the feasible set:

1. Compute:

   $$ y_k = x_k - \gamma_k g_k, \quad g_k \in \partial f(x_k). $$

2. Project onto $X$:

   $$ x_{k+1} = P_X(y_k) = P_X(x_k - \gamma_k g_k). $$

This ensures that all iterates remain feasible.

A key inequality that governs <strong><span style="color: var(--color-aqua);">convergence</span></strong>:

$$ \|x_{k+1} - x^*\| \leq \|x_k - x^*\|^2 - 2 \gamma_k ( f(x_k) - f^{\text{opt}}) + \gamma_k^2 \|g_k\|^2. $$

This highlights the **trade-off** between step size, function values, and convergence.

---

## Summary

- **Subgradient descent** is an extension of gradient descent for **non-smooth convex functions**.
- The step size **must be carefully chosen** to ensure convergence.
- **Convergence is slower** compared to gradient descent but still guaranteed.
- **Projected subgradient methods** allow constrained optimization by keeping iterates within a feasible set.

This method is widely used in **non-smooth optimization problems**, such as **L1-regularized learning (Lasso)** and **robust optimization**.
