---
{"dg-publish":true,"dg-path":"KSE/Optimization Techniques/14. PROXIMAL GRADIENT DESCENT.md","permalink":"/kse/optimization-techniques/14-proximal-gradient-descent/","tags":["kse","math/calculus"],"created":"2025-03-10T09:46:03.417+02:00","updated":"2025-03-10T10:56:27.394+02:00"}
---


# 14. PROXIMAL GRADIENT DESCENT

---

**Proximal Gradient Descent** is an extension of [[02 - KSE/2_5-term/Methods of Optimization/11. GRADIENT DESCENT\|gradient descent]] for optimizing **composite functions** that consist of a smooth function and a possibly <strong><span style="color: var(--color-orange);">non-smooth function</span></strong>. It generalizes gradient descent by incorporating a proximal step that accounts for non-smooth regularization terms.

---

## Composite Optimization Problems

We consider optimization problems of the form:

$$
f(x) := g(x) + h(x)
$$

where:

- $g(x)$ is a **[[02 - KSE/2_5-term/Methods of Optimization/06. Smooth Functions\|smooth]]** function (e.g., differentiable with an **[[02 - KSE/2_5-term/Methods of Optimization/07. Lipschitz Functions#Lipschitz Gradient\|L-Lipschitz gradient]]**).
- $h(x)$ may <strong><span style="color: var(--color-orange);">not be differentiable</span></strong> but is convex.

<strong><span style="color: var(--color-aqua);">The challenge: </span></strong>

- How do we minimize $f(x)$ when $h(x)$ is not smooth?
- We solve this using the **proximal gradient descent method**!

---

## Idea of Proximal Gradient Descent

The standard gradient descent update for minimizing a smooth function $g(x)$ is:

$$
x_{t+1} = \arg\min_{y \, \in \, \mathbb{R}^n} \left( g(x_t) + \nabla g(x_t)^T (y - x_t) + \frac{1}{2\gamma} \|y - x_t\|^2 \right)
$$

For composite functions $f(x) = g(x) + h(x)$, we modify the update step to include $h(x)$:

$$
x_{t+1} = \arg\min_{y \, \in \, \mathbb{R}^n} \left( g(x_t) + \nabla g(x_t)^T (y - x_t) + \frac{1}{2\gamma} \|y - x_t\|^2 + \textcolor{var(--color-orange)}{h(y)} \right)
$$

We just added the non-smooth term $\textcolor{var(--color-orange)}{h(y)}$ to the objective function.

Rewriting this, we get the proximal gradient descent update:

$$
x_{t+1} = \arg\min_{y \, \in \, \mathbb{R}^n} \left( \frac{1}{2\gamma} \|y - (x_t - \gamma \nabla g(x_t))\|^2 + \textcolor{var(--color-orange)}{h(y)} \right)
$$

> [!summary] Note!
> Here is the explanation of this transformation: [[02 - KSE/2_5-term/Methods of Optimization/Proximal GD Idea Explained\|Proximal GD Idea Explained]]

---

## Proximal Gradient Descent Algorithm

An iteration of proximal gradient descent is defined as:

$$
x^{t+1} := \text{prox}_{h, \gamma} \left( x^t - \gamma \nabla g(x^t) \right)
$$

where $\text{prox}_{h, \gamma}$ is the proximal mapping for a given function $h$ and parameter $\gamma > 0$.

### Steps

1. **Gradient Descent Step**:
   Compute $z = x_t - \gamma \nabla g(x_t)$ (just like in [[02 - KSE/2_5-term/Methods of Optimization/11. GRADIENT DESCENT\|gradient descent]])
2. Proximal Minimization:

   Compute the proximal operator:

   $$
   x_{t+1} = \arg\min_y \left( \frac{1}{2\gamma} \|y - z\|^2 + h(y) \right)
   $$

This step ensures that $x_{t+1}$ remains **close to $z$** while also incorporating the<strong><span style="color: var(--color-orange);"> non-smooth term $h(y)$</span></strong>.

---

## Proximal Gradient Descent as a Generalization of Gradient Descent

Proximal gradient descent recovers **[[02 - KSE/2_5-term/Methods of Optimization/11. GRADIENT DESCENT\|basic gradient descent]]** and **[[02 - KSE/2_5-term/Methods of Optimization/13. PROJECTED GRADIENT DESCENT\|projected gradient descent]]** as special cases:

- If $h \equiv 0$, we recover <strong><span style="color: var(--color-pink);">gradient descent</span></strong>.
- If $h \equiv \iota_X$ (the <strong><span style="color: var(--color-aqua);">indicator function</span></strong> of a convex set $X$), we recover <strong><span style="color: var(--color-cyan);">projected gradient descent</span></strong>, where:

  - The indicator function $\iota_X(x)$ is defined as:

    $$
    \iota_X(x) :=
    \begin{cases}
    0, & x \in X \\
    +\infty, & x \notin X
    \end{cases}
    $$

  - The proximal mapping simplifies to a projection onto $X$:

    $$
    \text{prox}_{h, \gamma}(z) := \arg\min_{y} \left\{ \frac{1}{2\gamma} \|y - z\|^2 + \iota_X(y) \right\} = \arg\min_{y \, \in \, X} \|y - z\|^2
    $$

---

## Convergence Rates of Proximal Gradient Descent

The convergence of proximal gradient descent follows the same principles as gradient descent, now extended to non-smooth functions $h(x)$.

If $g(x)$ is **[[02 - KSE/2_5-term/Methods of Optimization/03. Convex Functions\|convex]]** and **[[02 - KSE/2_5-term/Methods of Optimization/06. Smooth Functions\|L-smooth]]**, and we set:

$$
\gamma_k = \frac{1}{L},
$$

then proximal gradient descent satisfies:

$$
f(x_k) - f^* \leq \frac{L}{2k} \|x_0 - x^*\|^2.
$$

This shows that proximal gradient descent converges at a rate of $O(\frac{1}{k})$, similar to **[[02 - KSE/2_5-term/Methods of Optimization/11. GRADIENT DESCENT#Theorem 3 Smooth and Convex Functions\|standard gradient descent]]** for convex and smooth functions.

---

## Summary

- Proximal gradient descent extends gradient descent to composite functions $f(x) = g(x) + h(x)$.
- It consists of a gradient step for $g(x)$ and a proximal step for $h(x)$.
- It generalizes gradient descent (when $h(x) = 0$) and projected gradient descent (when $h(x)$ is an indicator function).
- Convergence is similar to gradient descent for smooth functions.

This method is widely used in sparse learning, compressed sensing, and machine learning, where non-smooth regularization terms (e.g., L1-norm in Lasso regression) play a key role in inducing sparsity.
