---
{"dg-publish":true,"dg-path":"KSE/Optimization Techniques/10. Existence of a Minimum.md","permalink":"/kse/optimization-techniques/10-existence-of-a-minimum/","tags":["kse","math/calculus"],"created":"2025-03-09T20:51:52.981+02:00","updated":"2025-03-09T21:32:39.263+02:00"}
---


# Existence of a Minimum

---

Understanding when a function reaches a minimum at a given point is crucial in optimization. These conditions are divided into **necessary** and **sufficient** conditions, classified based on the **first-order** and **second-order derivatives** of the function.

---

## First-Order Necessary Condition (FONC)

If a function $f$ is continuously differentiable and has a **local minimum** at $x^*$, then its **gradient must satisfy**:

$$
d^T \nabla f(x^*) \geq 0
$$

for any **feasible (possible) direction** $d$ at $x^*$. This condition ensures that the function <strong><span style="color: var(--color-pink);">does not decrease</span></strong> in any feasible direction.

### Special Case: Interior Minimum

If $x^*$ is an **interior point** of the domain $\Omega$, then the condition simplifies to:

$$
\nabla f(x^*) = 0
$$

This means that **at a local minimum in an unconstrained setting, the gradient must be zero**.

---

## Second-Order Necessary Condition (SONC)

If $f$ is **twice continuously differentiable** and $x^*$ is a **local minimizer**, then:

1. The **first-order condition** holds:
   $$
   d^T \nabla f(x^*) = 0
   $$
2. The **Hessian (second derivative test) must be positive semidefinite**:
   $$
   d^T H_f(x^*) d \geq 0
   $$

where $H_f(x^*)$ is the Hessian matrix of $f$. This ensures that the function is **not curving downward in any feasible direction** at $x^*$.

### Special Case: Interior Minimum

If $x^*$ is an **interior point**, then:

- The gradient **must be zero**:
  $$
  \nabla f(x^*) = 0
  $$
- The Hessian **must be positive semidefinite**:
  $$
  d^T H_f(x^*) d \geq 0 \quad \forall d \in \mathbb{R}^n.
  $$

This means the function **curves upward or remains flat in all directions** at $x^*$.

---

## Second-Order Sufficient Condition (SOSC). Interior Case

For **$x^*$ to be a strict local minimizer**, a stronger condition must hold:

1. **Gradient is zero**:
   $$
   \nabla f(x^*) = 0
   $$
2. **Hessian is positive definite**:
   $$
   H_f(x^*) > 0.
   $$

This ensures that the function **strictly curves upwards**, confirming $x^*$ as a **strict local minimum**.

---

## Examples and Counterexamples

**$f(x) = x^3$ at $x = 0$**

<strong><span style="color: var(--color-green);">Satisfies FONC and SONC</span></strong>, but is <strong><span style="color: var(--color-red);">not a minimum</span></strong>!

![FONC and SONC 1.png](/img/user/assets/FONC%20and%20SONC%201.png)

**$f(x_1, x_2) = x_1^2 - x_2^2$ at $(0,0)$**

<strong><span style="color: var(--color-green);">Satisfies FONC</span></strong> but <strong><span style="color: var(--color-orange);">not SONC</span></strong>, meaning $(0,0)$ is <strong><span style="color: var(--color-red);">not a minimum</span></strong>!

![FONC and SONC 2.png](/img/user/assets/FONC%20and%20SONC%202.png)

**$f(x_1, x_2) = x_1^2 + x_2^2$ at $(0,0)$**

<strong><span style="color: var(--color-green);">Satisfies FONC and SOSC</span></strong>, making $(0,0)$ a <strong><span style="color: var(--color-aqua);">strict local minimum</span></strong>!

![SOSC.png](/img/user/assets/SOSC.png)

---

## Summary

| Condition                          | Formula                    | Interpretation                            |
| ---------------------------------- | -------------------------- | ----------------------------------------- |
| **FONC** (First-order necessary)   | $d^T \nabla f(x^*) \geq 0$ | Ensures no descent direction at $x^*$.    |
| **SONC** (Second-order necessary)  | $d^T H_f(x^*) d \geq 0$    | Ensures function is not curving downward. |
| **SOSC** (Second-order sufficient) | $H_f(x^*) > 0$             | Ensures strict local minimum.             |

These conditions form the **foundation of optimization theory**, guiding both theoretical analysis and practical algorithms. ðŸš€
