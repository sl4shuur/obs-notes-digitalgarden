---
{"dg-publish":true,"dg-path":"KSE/Optimization Techniques/04. Epigraph. Jensenâ€™s Inequality.md","permalink":"/kse/optimization-techniques/04-epigraph-jensen-s-inequality/","tags":["kse","math/calculus"],"created":"2025-02-17T20:23:14.210+02:00","updated":"2025-03-09T16:14:49.522+02:00"}
---


# Epigraph of a Function. Jensenâ€™s Inequality

---

## Epigraph of a convex function

To understand [[02 - KSE/2_5-term/Methods of Optimization/03. Convex Functions\|convexity]] deeply, we look at the **epigraph** of a function.

<strong><span style="color: var(--color-cyan);">We already know this:</span></strong>
The graph of a function $f(x) \colon \mathbb{R}^n \to \mathbb{R}$ is the set of all points $(x, f(x))$ in the domain of $f$. It's defined as:

$$
\{(\mathbf{x}, f(\mathbf{x})) \mid \mathbf{x} \in D(f)\}
$$

where $D(f)$ is the domain of $f$.

**What is the epigraph?**  
The **epigraph** of a function $f$ is the set of all points that lie on or above its graph:

$$
\text{epi}(f) = \{ (\mathbf{x}, \alpha) \mid \mathbf{x} \in D(f), \alpha \geq f(\mathbf{x}) \}
$$

![Epigraphs.png|600](/img/user/assets/Epigraphs.png)

<strong><span style="color: var(--color-aqua);">Key property:</span></strong>
A **function is convex** if and only if its <strong><span style="color: var(--color-cyan);">epigraph is a convex set</span></strong>.

This means that if you look at the "shadow" or "area above" the curve, it has no dents or dips â€” just a smooth "roof" shape.

---

## Jensenâ€™s inequality: The fingerprint of **convexity**

> [!tip] **[[02 - KSE/2_5-term/Methods of Optimization/03. Convex Functions#What Is a Convex Function?\|Recall]]**
> For **convex functions**, the graph lies below every chord between points:
> $f(\lambda x_1 + (1 - \lambda) x_2) \leq \lambda f(x_1) + (1 - \lambda) f(x_2)$

Jensenâ€™s inequality is a powerful rule that applies to all convex functions. It says:

$$
f\left( \sum_{i=1}^{n} \lambda_i x_i \right) \leq \sum_{i=1}^{n} \lambda_i f(x_i)
$$

where:

- $\lambda_i \geq 0$ for all $i$ (weights are non-negative)
- $\sum_{i=1}^{n} \lambda_i = 1$ (weights sum to 1)
- $\mathbf{x}_i$ are input with index $i$ for the function $f$. $\mathbf{x}_i$ can be vectors of any dimension.

<strong><span style="color: var(--color-aqua);">What does Jensenâ€™s inequality mean?</span></strong>  
It means that if you **average inputs** before applying the function, the result is always better (or equal) than **averaging outputs** after applying the function.

So, previously we had $i = 2$ and formula was:

$$
\begin{aligned}
f(\lambda x_1 + (1 - \lambda) x_2) &\leq \lambda f(x_1) + (1 - \lambda) f(x_2) \\
f\left( \sum_{i=1}^{2} \lambda_i x_i \right) &\leq \sum_{i=1}^{2} \lambda_i f(x_i)
\end{aligned}
$$

---

### Operations that preserve convexity

Convexity is stable â€” it is not easy to "break" a convex function. Here are two rules that help build new convex functions from old ones:

1. **Sum of convex functions is convex:**  
   If $f_1, f_2, \dots, f_k$ are convex and $\lambda_1, \lambda_2, \dots, \lambda_k \geq 0$, then:

   $$
   f(x) = \sum_{i=1}^{k} \lambda_i f_i(x) \quad \text{is convex}
   $$

   **Why?** The "average" of several bowls is still a bowl.

2. **Composition with affine functions is convex:**  
   If $f$ is convex and $g(x) = Ax + b$ is an affine function, then:
   $$
   h(x) = f(g(x)) = f(Ax + b) \quad \text{is convex}
   $$
   **Why?** Affine transformations only "tilt" or "shift" the bowl â€” they cannot create dents or dips.

---

### Summary of key ideas about convex functions

| Concept                         | Meaning                                                        |
| ------------------------------- | -------------------------------------------------------------- |
| Convex function                 | Graph lies below every chord between points.                   |
| Strictly convex function        | Graph lies strictly below chords â€” ensures a unique minimum.   |
| Epigraph                        | The region above the graph â€” convex if the function is convex. |
| Jensenâ€™s inequality             | Blending inputs is always better than blending outputs.        |
| Gradient condition              | Tangent lines stay below the graph.                            |
| Hessian condition               | Positive curvature (Hessian positive semidefinite).            |
| Convexity-preserving operations | Sums and affine transformations preserve convexity.            |

---

### Final intuition: Why convex functions are powerful

Think of a **bowl-shaped landscape**:

- **Anywhere you drop a ball, it rolls down to the same lowest point** â€” the global minimum.
- There are **no traps or false valleys**, just one solution.
- The **path to the lowest point** is always the **fastest and shortest**.

Thatâ€™s why optimization problems love convex functions â€” they guarantee a **clear, simple route to the best solution.** ðŸš€
