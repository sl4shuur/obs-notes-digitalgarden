---
{"dg-publish":true,"dg-path":"KSE/Optimization Techniques/16. STOCHASTIC GRADIENT DESCENT.md","permalink":"/kse/optimization-techniques/16-stochastic-gradient-descent/","tags":["kse","math/calculus"],"created":"2025-03-10T09:57:17.040+02:00","updated":"2025-03-11T21:42:11.291+02:00"}
---


# STOCHASTIC GRADIENT DESCENT

---

## Stochastic Gradient Descent (SGD)

In many optimization problems, the objective function is structured as a **sum** over individual components:

$$
f(x) = \frac{1}{m} \sum_{i=1}^{m} f_i(x).
$$

where each $f_i$ represents the cost associated with an observation in a dataset of size $m$.

**Computing the full gradient** $\nabla f(x)$ for large datasets can be <strong><span style="color: var(--color-red);">computationally expensive</span></strong> ğŸ˜Ÿ

To address this, we use **Stochastic Gradient Descent (SGD)**.

---

## Stochastic Gradient Descent Algorithm

The SGD update rule is:

1. **Initialize** $x_0 \in \mathbb{R}^n$
2. **For** $k = 0,1,2, \dots$

   - Randomly sample **one** index $i_k \in \{1, \dots, m\}$
   - Compute the stochastic gradient $\nabla f_{i_k}(x_k)$
   - Update:

   $$
   x_{k+1} = x_k - \gamma_k \nabla f_{i_k}(x_k)
   $$

   where **$\gamma_k > 0$ is the step size (learning rate).**

ğŸ’¡ <strong><span style="color: var(--color-aqua);">Key Idea:</span></strong>

Instead of using the full gradient, we update using **only one random** $f_i$ at each step!

ğŸ”¹ **Advantage:** Faster updates  
ğŸ”¹ **Disadvantage:** Noisy updates (fluctuations)

ğŸ“Œ **Only update with the gradient of $f_i$!**

---

## Expected Gradient in SGD

We define the **stochastic gradient**:

$$
g_k := \nabla f_{i_k}(x_k).
$$

On expectation, this gives:

$$
\mathbb{E}[g_k | x_k] = \frac{1}{m} \sum_{i=1}^{m} \nabla f_i(x) = \nabla f(x).
$$

Thus, **SGD provides an unbiased estimate of the true gradient**.

Using the **Partition Theorem**, we get:

$$
\mathbb{E}[g_k^T (x_k - x^*)] = \mathbb{E}[\nabla f(x_k)^T (x_k - x^*)] \geq \mathbb{E}[f(x_k) - f(x^*)].
$$

âœ” <strong><span style="color: var(--color-aqua);">Conclusion:</span></strong>
A lower bound holds in expectation!

---

## Convergence of SGD with Bounded Gradients

<strong><span style="color: var(--color-purple);">Theorem</span></strong>  
Let $f: \mathbb{R}^n \to \mathbb{R}$ be **[[02 - KSE/2_5-term/Methods of Optimization/03. Convex Functions\|convex]] and differentiable** with a global minimum $x^*$.  
Assume:

- $\|x_0 - x^*\| \leq R$
- The stochastic gradient is bounded: $\mathbb{E}[\|g_k\|^2] \leq B^2$

Choosing a **constant step size**:

$$
\gamma := \frac{R}{B\sqrt{T}}
$$

The **SGD error bound**:

$$
\frac{1}{K} \sum_{k=0}^{K-1} \mathbb{E}[f(x_k) - f(x^*)] \leq \frac{RB}{\sqrt{K}}.
$$

ğŸ”¹ **Implication:** SGD has a **sublinear convergence rate** $O(\frac{1}{\sqrt{K}})$.

---

## SGD with Strong Convexity

If $f$ is **[[02 - KSE/2_5-term/Methods of Optimization/06. Smooth Functions#Strongly Convex Functions (Another Approach)\|strongly convex]]** with parameter $\mu > 0$, and we use a **decreasing step size**:

$$
\gamma_k := \frac{2}{\mu(k+1)}
$$

then **SGD achieves a faster rate**:

$$
\mathbb{E} \left[ f\left( \frac{2}{K(K+1)} \sum_{k=1}^{K} k x_k \right) - f(x^*) \right] \leq \frac{2 B^2}{\mu (K+1)}.
$$

where $B^2 := \max_{k=1}^{T} \mathbb{E} \left[ \| g_k \|^2 \right]$.

Almost same result as for [[02 - KSE/2_5-term/Methods of Optimization/15. SUBGRADIENT DESCENT\|subgradient descent]], but **in expectation**.

ğŸš€ <strong><span style="color: var(--color-green);">Faster Convergence!</span></strong> ğŸš€

---

## Stochastic Subgradient Descent

For **non-differentiable** functions, we replace the gradient with a **subgradient**:

$$
g_k \in \partial f_{i_k}(x_k).
$$

The update rule remains:

$$
x_{k+1} = x_k - \gamma_k g_k.
$$

ğŸ“Œ **Works even when $f(x)$ is not smooth!**

---

## Projected Stochastic Gradient Descent

For **[[02 - KSE/2_5-term/Methods of Optimization/13. PROJECTED GRADIENT DESCENT\|constrained problems]]**, we project the update onto the feasible set $X$:

$$
x_{k+1} = P_X (x_k - \gamma_k g_k).
$$

where $P_X$ is the **[[02 - KSE/2_5-term/Methods of Optimization/09. Projections\|projection operator]]**. This is called **Projected SGD**.

---

## Summary

| **SGD Variant**     | **Update Rule**                                          | **Convergence Rate**     |
| ------------------- | -------------------------------------------------------- | ------------------------ |
| Standard SGD        | $x_{k+1} = x_k - \gamma_k \nabla f_{i_k}(x_k)$           | $O(\frac{1}{\sqrt{K}})$  |
| Strongly Convex SGD | $x_{k+1} = x_k - \frac{2}{\mu(k+1)} \nabla f_{i_k}(x_k)$ | $O(\frac{1}{K})$         |
| Subgradient SGD     | $x_{k+1} = x_k - \gamma_k g_k$                           | Works for non-smooth $f$ |
| Projected SGD       | $x_{k+1} = P_X(x_k - \gamma_k g_k)$                      | Constrained optimization |

ğŸ“Œ <strong><span style="color: var(--color-aqua);">Key Takeaways</span></strong>:
âœ” **SGD is computationally efficient** ğŸš€  
âœ” **Uses random gradient updates** instead of full dataset ğŸ“Š  
âœ” **Converges in expectation** (but with noise) ğŸ“‰  
âœ” **Stronger assumptions â†’ Faster convergence** ğŸ
