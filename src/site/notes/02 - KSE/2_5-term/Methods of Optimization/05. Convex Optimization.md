---
{"dg-publish":true,"dg-path":"KSE/Optimization Techniques/05. Convex Optimization.md","permalink":"/kse/optimization-techniques/05-convex-optimization/","tags":["kse","math/calculus"],"created":"2025-03-09T13:44:01.058+02:00","updated":"2025-03-09T16:14:33.183+02:00"}
---


# Convex Optimization

---

## Convex Optimization

Convex optimization deals with finding the <strong><span style="color: var(--color-cyan);">minimum</span></strong> of a function while ensuring that the search space remains well-behaved. The key requirement is that the **objective function** is convex and that the **feasible region** $\color{var(--color-yellow)}{\Omega}$ (the set of allowed values for $x$) is also convex. This guarantees that any local minimum is also a <strong><span style="color: var(--color-blue);">global minimum</span></strong>, making convex optimization highly efficient and predictable.

A [[02 - KSE/2_5-term/Methods of Optimization/Optimization Formula\|convex optimization problem]] is formally written as:

$$
\min f(x), \quad x \in \Omega \subseteq \mathbb{R}^n
$$

where:

- $f$ is a **convex function** (curves upwards like a bowl).
- $\Omega \subseteq D(f)$ is a **convex set**, meaning that if two points belong to the set, the entire line segment connecting them is also inside the set.

---

## Local Minimum in Convex Optimization

A **local minimum** of a function $f: D(f) \to \mathbb{R}$ is a point $x_*$, where the function value is smaller than or equal to all nearby values within a small distance$\delta$:

$$
\exists \delta > 0: \quad f(x_*) \leq f(y) \quad \forall y \in D(f) \text{ satisfying } \| x_* - y \| < \delta.
$$

This means that in a small neighborhood around $x_*$ no other points have a lower function value.

![Local Minima 2D.png|500](/img/user/assets/Local%20Minima%202D.png)

![Local Minima 3D.png|500](/img/user/assets/Local%20Minima%203D.png)

## Local Minima are Global Minima

One of the most powerful properties of convex functions is that **any local minimum is automatically a global minimum**.

### Lemma 1

If $x_*$ is a local minimum of a **convex function**, then it is also a **global minimum**, meaning:

$$
f(x_*) \leq f(y) \quad \forall y \in D(f).
$$

This property makes convex optimization particularly efficient because it eliminates the risk of getting trapped in **local minima**, unlike in non-convex problems.

![Non-Convex Minima.png|500](/img/user/assets/Non-Convex%20Minima.png)
_In non-convex functions, local minima can be **traps** that prevent finding the global minimum._

---

## Critical Points are Global Minima

A **critical point** of a function is where the gradient (first derivative) is zero. In the case of convex functions, a critical point is always a **global minimum**.

### Lemma 2

If $f$ is convex and **differentiable** over an open domain $D(f)$, then any point $x \in D(f)$ where the gradient is zero is a **global minimum**:

$$
\nabla f(x) = 0 \quad \Rightarrow \quad x \text{ is a global minimum}.
$$

This means that solving the equation $\nabla f(x) = 0$ is sufficient to find the global minimum in convex optimization problems.

<strong><span style="color: var(--color-aqua);">Geometric intuition:</span></strong>
The **tangent hyperplane** (a generalization of a tangent line in higher dimensions) is **horizontal** at $x$, meaning the function stops decreasing and reaches its minimum.

![gradient is zero.png|500](/img/user/assets/gradient%20is%20zero.png)
$f(x_1, x_2) = x_1^2 + x_2^2 - 9$ has a global minimum at $(0, 0)$ where the gradient is zero.

$\nabla f(x) = \begin{bmatrix} 2x_1 \\ 2x_2 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$

---

## Strictly Convex Functions

A [[02 - KSE/2_5-term/Methods of Optimization/03. Convex Functions#Strictly Convex Functions\|strictly convex function]] is a convex function where the inequality is **strict**â€”the function curves more sharply, ensuring that it has **only one** minimum.

> [!tip]+ Recall from the image:
> ![Strict convex.png](/img/user/assets/Strict%20convex.png)

### Lemma 3

If $f: D(f) \to \mathbb{R}$ is **strictly convex**, then it has at most **one global minimum**.

This property is useful because it guarantees **uniqueness**, meaning that optimization algorithms will always find **the same solution**.

---

## Summary

| Concept                       | Meaning                                                                                   |     |
| ----------------------------- | ----------------------------------------------------------------------------------------- | --- |
| **Convex optimization**       | Minimizing a convex function over a convex set ensures efficient and predictable results. |     |
| **Local minimum**             | A point where$f(x_*) \leq f(y)$for nearby$y$.                                             |     |
| **Local = Global**            | In convex functions, every **local minimum is a global minimum**.                         |     |
| **Critical points**           | If $\nabla f(x) = 0$ in a convex function, then$x$is a **global minimum**.                |     |
| **Strictly convex functions** | Have at most **one global minimum**, ensuring uniqueness.                                 |     |

Convex optimization provides a **powerful and reliable framework** for finding optimal solutions in various fields, from machine learning to economics. ðŸš€
