---
{"dg-publish":true,"dg-path":"KSE/Optimization Techniques/11. GRADIENT DESCENT.md","permalink":"/kse/optimization-techniques/11-gradient-descent/","tags":["kse","math/calculus"],"created":"2025-03-10T08:08:07.037+02:00","updated":"2025-03-10T08:29:34.539+02:00"}
---


# GRADIENT DESCENT

---

Gradient descent is an iterative optimization algorithm used to find the minimum of a differentiable function. Given a function $f(x):\mathbb{R}^n \to \mathbb{R}$ that is [[02 - KSE/2_5-term/Methods of Optimization/03. Convex Functions\|convex]] and differentiable, has a **global minimum $x^*$**. So, we seek to find a point $\hat{x}$ such that:

$$
|f(\hat{x}) - f(x^*)| \leq \varepsilon.
$$

> [!Remark] Remark
> We need to find such a point $\hat{x}$ that minimizes the function $f(x)$ within a given tolerance $\varepsilon$.
> So, the goal is to find the point $\hat{x}$ that is very close to the global minimum $x^*$.

The key idea is to start from an **initial point** $x^0 \in \mathbb{R}^n$ and iteratively **update** it using the gradient information to generate a sequence $\{x^k\}_{k=0,1,2,\dots}$ that satisfies:

$$
f(x^{k+1}) < f(x^k).
$$

---

## Iterative Algorithm

1. **Choose an initial point** $x^0 \in \mathbb{R}^n$.
2. **Update rule**: The next point is computed using the gradient:

   $$
   x^{k+1} = x^k - \gamma \nabla f(x^k).
   $$

   Here, $\gamma$ is the **step size** (<strong><span style="color: var(--color-pink);">learning rate</span></strong>).

3. **Repeat** this process until a **stopping criterion** is satisfied.

---

## Geometric Interpretation

- Gradient descent follows the steepest **descent direction** to minimize the function.
- In a **3D bowl-like function**, each iteration moves the point **downhill** until reaching the minimum.
- The algorithm follows **contour lines** in 2D, converging towards the optimal point.

![gradient descent.png|700](/img/user/assets/gradient%20descent.png)

---

## Average Error in Gradient Descent

Over the first $K$ iterations, the error satisfies:

$$
\sum_{k=0}^{K-1} (f(x^k) - f(x^*)) \leq \frac{\gamma}{2} \sum_{k=0}^{K-1} \|\nabla f(x^k)\|^2 + \frac{1}{2\gamma} \|x^0 - x^*\|^2.
$$

### Step Size Problems

- If $\gamma$ is **too small**, the algorithm converges **slowly**.
- If $\gamma$ is **too large**, the algorithm may **overshoot** and fail to converge.

---

## Choosing the Step Size

### Theorem 1: Bounded Gradient

For a convex and differentiable function $f(x)$ with a global minimum $x^*$, and assuming:

$$
\|x - x^*\| \leq R, \quad \|\nabla f(x)\| \leq B, \quad \forall x.
$$

Where $B$ is the [[02 - KSE/2_5-term/Methods of Optimization/07. Lipschitz Functions#Definition\|Lipschitz constant of the gradient]].

Choosing the step size:

$$
\gamma = \frac{R}{B\sqrt{K}},
$$

yields:

$$
\frac{1}{K} \sum_{k=0}^{K-1} (f(x^k) - f(x^*)) \leq \frac{RB}{\sqrt{K}}.
$$

Thus, the <strong><span style="color: var(--color-red);">average error decreases</span></strong> as <strong><span style="color: var(--color-aqua);">$O(\frac{1}{\sqrt{K}})$</span></strong>

---

### Theorem 2: Smooth Functions

If $f(x)$ is **differentiable and [[02 - KSE/2_5-term/Methods of Optimization/06. Smooth Functions\|smooth]]** with parameter $L$, using the step size:

$$
\gamma = \frac{1}{L},
$$

gradient descent satisfies:

$$
f(x^{k+1}) \leq f(x^k) - \frac{1}{2L} \|\nabla f(x^k)\|^2.
$$

So, the <strong><span style="color: var(--color-red);">function value decreases</span></strong> at each iteration.

---

### Theorem 3: Smooth and Convex Functions

For a **[[02 - KSE/2_5-term/Methods of Optimization/03. Convex Functions\|convex]] and differentiable function** with [[02 - KSE/2_5-term/Methods of Optimization/06. Smooth Functions#Definition\|smoothness]] parameter $L$, choosing:

$$
\gamma = \frac{1}{L},
$$

ensures:

$$
f(x^k) - f(x^*) \leq \frac{L}{2K} \|x^0 - x^*\|^2.
$$

This means the function value **converges** to the minimum at a rate of <strong><span style="color: var(--color-aqua);">$O(\frac{1}{K})$</span></strong>

---

## Stopping Criteria

To stop the iteration process, we use one of the following conditions:

1. **Gradient norm is small**:

   $$
   \max_i \left| \frac{\partial f}{\partial x_i} \right| < \varepsilon.
   $$

2. **Sum of gradient components is small**:

   $$
   \|\nabla f(x^k)\|^2 = \sum_{i=1}^{n} \left( \frac{\partial f}{\partial x_i} \right)^2 < \varepsilon.
   $$

3. **Function values stop decreasing**:

   $$
   |f(x^{k+1}) - f(x^k)| < \varepsilon.
   $$

These conditions ensure that the algorithm stops when the function is close to the minimum.

---

## Summary

- **Gradient descent** iteratively moves in the **direction of steepest descent**.
- **Step size $\gamma$** determines the trade-off between **speed** and **stability**.
- **Theoretical results** guarantee convergence under appropriate conditions.
- **Stopping criteria** ensure we do not perform unnecessary iterations.

Gradient descent is widely used in **machine learning, optimization, and deep learning** due to its simplicity and efficiency! ðŸš€
